{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4229d26429f6fdf9dd42b26b8b02a4c4175b707"
   },
   "source": [
    "# CNN implementation on MNIST dataset in Keras- Detailed explanation  \n",
    "  <hr>\n",
    "First version - 25/07/2018  \n",
    "Last version - 24/11/2018\n",
    "<hr>\n",
    "  \n",
    "1. **Introduction**  \n",
    "  \n",
    "2. **Data pre-processing**  \n",
    "     2.1. Load data  \n",
    "     2.2. Check shape, data type  \n",
    "     2.3. Extract xtrain, ytrain  \n",
    "     2.4. Mean and std of classes  \n",
    "     2.5. Check nuls and missing values  \n",
    "     2.6. Check nuls and missing values  \n",
    "     2.6. Visualization  \n",
    "     2.7. Normalization  \n",
    "     2.8. Reshape  \n",
    "     2.9. One hot encoding of label  \n",
    "     2.10. Split training and validation sets    \n",
    "      \n",
    "3. **CNN**  \n",
    "    3.1. Define model architecture  \n",
    "    3.2. Compile the moedl  \n",
    "    3.3. Set other parameters   \n",
    "    3.4. Fit model  \n",
    "    3.5. Plot loss and accuracy  \n",
    "    3.6. Plot confusion matrix  \n",
    "    3.7. Plot errors  \n",
    "  \n",
    "4. **Predict and save to csv**  \n",
    "  \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5780ab91c4dbd65ba303299ef03e596f55fff5c"
   },
   "source": [
    "# 1. **Introduction**\n",
    "\n",
    "This is my first CNN kernel and as such, I believe the [Digit Recognizer dataset/competition](https://www.kaggle.com/c/digit-recognizer) is a very suitable set of images for a beginner CNN project, considering the image size is homogeneous across all images (not common in real-world problems), that the size is small (28x28) so no resizing required, they are in grayscale and they are already in a csv, which can be easily read into a dataframe. \n",
    "\n",
    "<img src=\"https://www.codeproject.com/KB/AI/1233183/dMRUT6k.png\" ></img>\n",
    "\n",
    "Given the comfort that this dataset provides and taking inspration from very popular kernels such as [yassineghouzam's kernel](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6) and [poonaml's kernel](https://www.kaggle.com/poonaml/deep-neural-network-keras-way) among others, I've created my own kernel joining what I have found most useful from each kernel, as well as adding what I have learnt in the process and other notes that may be helpful for others or for future me.\n",
    "\n",
    "The kernel consists in 3 main parts:\n",
    "   * *Data preparation*: Firstly, even if the input data is already quite clean as mentioned before, it still needs some preparation and pre-processing in order to be in an appropriate format to then later be fed to the NN. This includes data separation, reshaping and visualization which might give insight to the data scientist as to the nature of the images.\n",
    "   * *CNN*: Afterwards, the NN is defined (this is where Keras comes in), the convolutional steps added, NN parameters initialized, and the model trained. This part takes the most time in a ML project.\n",
    "   * *Evaluation*: Once the model is trained, it's interesting to evaluate the model performance by seeing the progress of the loss and extract some conclusions, that the model is overfitting, or if there is high variance for instance.\n",
    "\n",
    "If you find some errors in theoretical concepts, comments of any kind or suggestions, please do let me know :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np # linear algebra, matrix multiplications\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# 2. **Data pre-processing**\n",
    "## 2.1 Load data\n",
    "\n",
    "   * *train*:  \n",
    "this is the data used to train the CNN.  \n",
    "the image data and their corresponding class is provided.   \n",
    "the CNN learns the weights to create the mapping from the image data to their corresponding class.  \n",
    "\n",
    "\n",
    "   * *test*:  \n",
    "this is the data used to test the CNN.  \n",
    "only the image data is provided.  \n",
    "the prediction is submitted to the competition and depending on the accuracy, a score is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "5c4c2fa7eeffa51a4c2294cac109bed54fda7dca"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./input/train.csv\")\n",
    "test = pd.read_csv(\"./input/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6dca7aa2c39302f7f2e568c01e7d44ecbf12dc1a"
   },
   "source": [
    "## 2.3. Check shape, data type\n",
    "\n",
    "   * *train*:  \n",
    "the train dataframe contains data from 42k images.  \n",
    "the data from each image is streched out in 1D with 28*28 = 784 pixels.  \n",
    "the first column is the label/class it belongs to, the digit it represents.  \n",
    "\n",
    "\n",
    "   * *test*:  \n",
    "the test dataframe contains data from 28k images.  \n",
    "this data shall be fed to the CNN so that it's new data, that the CNN has never seen before.  \n",
    "same as in the train dataset, image data is streched out in 1D with 784 pixels.  \n",
    "there is no label information, that is the goal of the competition, predicting labels as well as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "8a7cb2975ffe4514b061c379a9bf8cc37c48f9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "(28000, 784)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "5      0       0       0       0       0       0       0       0       0   \n",
       "6      7       0       0       0       0       0       0       0       0   \n",
       "7      3       0       0       0       0       0       0       0       0   \n",
       "8      5       0       0       0       0       0       0       0       0   \n",
       "9      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "5       0  ...         0         0         0         0         0         0   \n",
       "6       0  ...         0         0         0         0         0         0   \n",
       "7       0  ...         0         0         0         0         0         0   \n",
       "8       0  ...         0         0         0         0         0         0   \n",
       "9       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "5         0         0         0         0  \n",
       "6         0         0         0         0  \n",
       "7         0         0         0         0  \n",
       "8         0         0         0         0  \n",
       "9         0         0         0         0  \n",
       "\n",
       "[10 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "ntrain = train.shape[0]\n",
    "\n",
    "print(test.shape)\n",
    "ntest = test.shape[0]\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "3ffd9d5ea1aa332b428fa668215c7c84c28f3dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label     int64\n",
      "pixel0    int64\n",
      "pixel1    int64\n",
      "pixel2    int64\n",
      "pixel3    int64\n",
      "dtype: object\n",
      "label     int64\n",
      "pixel0    int64\n",
      "pixel1    int64\n",
      "pixel2    int64\n",
      "pixel3    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# check data type\n",
    "print(train.dtypes[:5]) # all int64, otherwise do train = train.astype('int64')\n",
    "print(train.dtypes[:5]) # all int64, otherwise do test = test.astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c71f725e3a5fb0bcca79a6797f48556888797e7"
   },
   "source": [
    "## 2.4 Extract xtrain, ytrain \n",
    "The CNN will be fed xtrain and it will learn the weights to map xtrain to ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f9d2f465a2abfab92687cd5c08404f7e8aef8ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ytrain:  (42000,)\n",
      "The images are 28x28 squares.\n",
      "Shape of xtrain:  (42000, 784)\n"
     ]
    }
   ],
   "source": [
    "# array containing labels of each image\n",
    "ytrain = train[\"label\"]\n",
    "print(\"Shape of ytrain: \", ytrain.shape)\n",
    "\n",
    "# dataframe containing all pixels (the label column is dropped)\n",
    "xtrain = train.drop(\"label\", axis=1)\n",
    "\n",
    "# the images are in square form, so dim*dim = 784\n",
    "from math import sqrt\n",
    "dim = int(sqrt(xtrain.shape[1]))\n",
    "print(\"The images are {}x{} squares.\".format(dim, dim))\n",
    "\n",
    "print(\"Shape of xtrain: \", xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "6e5f24bd732ba81028c32812eccec43ce50db629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    4\n",
       "4    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4b74cdf8150c9a6a4e6910bfa0f1441162d4f19"
   },
   "source": [
    "## 2.5. Mean and std of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "6eb1fc174dc86ce277e2b8fc858ea1067c8174ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000,)\n",
      "<class 'pandas.core.series.Series'>\n",
      "1    4684\n",
      "7    4401\n",
      "3    4351\n",
      "9    4188\n",
      "2    4177\n",
      "6    4137\n",
      "0    4132\n",
      "4    4072\n",
      "8    4063\n",
      "5    3795\n",
      "Name: label, dtype: int64\n",
      "The mean amount of elements per class is 4200.0\n",
      "The standard deviation in the element per class distribution is 237.08929400825616\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "\n",
    "# plot how many images there are in each class\n",
    "sns.countplot(ytrain)\n",
    "\n",
    "print(ytrain.shape)\n",
    "print(type(ytrain))\n",
    "\n",
    "# array with each class and its number of images\n",
    "vals_class = ytrain.value_counts()\n",
    "print(vals_class)\n",
    "\n",
    "# mean and std\n",
    "cls_mean = np.mean(vals_class)\n",
    "cls_std = np.std(vals_class,ddof=1)\n",
    "\n",
    "print(\"The mean amount of elements per class is\", cls_mean)\n",
    "print(\"The standard deviation in the element per class distribution is\", cls_std)\n",
    "\n",
    "# 68% - 95% - 99% rule, the 68% of the data should be cls_std away from the mean and so on\n",
    "# https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule\n",
    "if cls_std > cls_mean * (0.6827 / 2):\n",
    "    print(\"The standard deviation is high\")\n",
    "    \n",
    "# if the data is skewed then we won't be able to use accurace as its results will be misleading and we may use F-beta score instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33027153afbdb5887955c8f39e29c000e374b573"
   },
   "source": [
    "> Summary\n",
    "\n",
    "Shape of *xtrain* is: (42000, 784)  \n",
    "Shape of *ytrain* is: (42000, )  \n",
    "Shape of *test* is: (28000, 784)  \n",
    "\n",
    "number of classes = 10, the distribution of the pictures per class has a mean of 4200 images and a std of 237 images.     \n",
    "The digit 1 has the most representation (4684 images) and the digit 5 the least (3795 images). This data can be seen by printing *vals_class*  \n",
    "This corresponds to a small standard deviation (5.64%) so there is no class imbalance. In case there was, other techniques would have to be considered but this is outside the scope of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b07fc7612d5ce54f82d9debdce7273fd9a3f735"
   },
   "source": [
    "## 2.6. Check nuls and missing values\n",
    "```python\n",
    "df.isnull()\n",
    "```\n",
    "returns a boolean df with true if value is NaN and false otherwise.  \n",
    "```python\n",
    "df.isnull().any()\n",
    "```\n",
    "returns a df with 1 col and ncol rows where each row says if there is a NaN value present in that col.  \n",
    "```python\n",
    "df.isnull().any().any()\n",
    "```\n",
    "returns a bool with True if any of the df.isnull().any() rows is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "03c7f67d1575a25008ebd244872a262f4a4495ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count       784\n",
      "unique        1\n",
      "top       False\n",
      "freq        784\n",
      "dtype: object\n",
      "There are no missing values\n",
      "\n",
      "count       784\n",
      "unique        1\n",
      "top       False\n",
      "freq        784\n",
      "dtype: object\n",
      "There are no missing values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_nan(df):\n",
    "    print(df.isnull().any().describe())\n",
    "    print(\"There are missing values\" if df.isnull().any().any() else \"There are no missing values\")\n",
    "\n",
    "    if df.isnull().any().any():\n",
    "        print(df.isnull().sum(axis=0))\n",
    "        \n",
    "    print()\n",
    "        \n",
    "check_nan(xtrain)\n",
    "check_nan(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c960d91af825d5cf5d291ef019b779876e5ccb06"
   },
   "source": [
    "## 2.7. Visualization\n",
    "\n",
    "The first nine images in the dataset (which are not ordered by digit) are plotted, just for visualization. There is only one color channel (grayscale) and moreover the pixels are binarized, meaning that hey are either black (with value 0) or white (255). This makes the classification problem easier. Imagine that the CNN received colored digits, either solid, gradient, or digits with many colors. Probably some part of the neural network would focus on learning to tell the digits apart by looking at the colors, when the actual difference between the digits is in their shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "9a22f34f81cb94af5787f06448e326058e61122d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "# convert train dataset to (num_images, img_rows, img_cols) format in order to plot it\n",
    "xtrain_vis = xtrain.values.reshape(ntrain, dim, dim)\n",
    "\n",
    "# https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html\n",
    "# subplot(2,3,3) = subplot(233)\n",
    "# a grid of 3x3 is created, then plots are inserted in some of these slots\n",
    "for i in range(0,9): # how many imgs will show from the 3x3 grid\n",
    "    plt.subplot(330 + (i+1)) # open next subplot\n",
    "    plt.imshow(xtrain_vis[i], cmap=plt.get_cmap('gray'))\n",
    "    plt.title(ytrain[i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2051502a7a5443f14679c836f5f3bfabd8d0bdac"
   },
   "source": [
    "## 2.8. Normalization\n",
    "Pixels are represented in the range [0-255], but the NN converges faster with smaller values, in the range [0-1] so they are normalized to this range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "76e6f0a067bb7cabf2cfa079a3917f2391693872"
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "xtrain = xtrain / 255.0\n",
    "test = test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e07e4cdd4acf0ae14bb0dcb633fd2e020ff3e3d6"
   },
   "source": [
    "## 2.9. Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "9685eb7e78736876af2f032c53dd2f2d2ea5382b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape, pixels are in 1D vector: (42000, 784)\n",
      "After reshape, pixels are a 28x28x1 3D matrix: (42000, 28, 28, 1)\n",
      "Previous shape, pixels are in 1D vector: (28000, 784)\n",
      "After reshape, pixels are a 28x28x1 3D matrix: (28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape of image data to (nimg, img_rows, img_cols, 1)\n",
    "def df_reshape(df):\n",
    "    print(\"Previous shape, pixels are in 1D vector:\", df.shape)\n",
    "    df = df.values.reshape(-1, dim, dim, 1) \n",
    "    # -1 means the dimension doesn't change, so 42000 in the case of xtrain and 28000 in the case of test\n",
    "    print(\"After reshape, pixels are a 28x28x1 3D matrix:\", df.shape)\n",
    "    return df\n",
    "\n",
    "xtrain = df_reshape(xtrain) # numpy.ndarray type\n",
    "test = df_reshape(test) # numpy.ndarray type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d94d64556e53d62e3822542f4575d82f1132bd28"
   },
   "source": [
    "> Note\n",
    "\n",
    "In real world problems, the dimensions of images could diverge from this particular 28x28x3 set in two ways:\n",
    "   * Images are usually much bigger\n",
    "\n",
    "In this case all images are 28x28x1, but in another problem I'm working on, I have images of 3120x4160x3, so much bigger and in RGB. Usually images are resized to much smaller dimensions, in my case I'm resizing them to 64x64x3 but they can be made much smaller depending on the problem. In this MNIST dataset there is no such problem since the dimensions are already small.\n",
    "   * Images don't usually have the same dimensions\n",
    "   \n",
    "Different dimension images are a problem since dense layers at the end of the CNN have a fixed number of neurons, which cannot be dynamically changed. This means that the layer expects fixed image dimensions, which means all images must be resized to the same dimensions before training. There is another option, namely, using a FCN (fully convoluted network) which consits solely of convolutional layers and a very big pooling in the end, so each image can be of any size, but this architecture isn't as popular as the CNN + FC (fully connected) layers which is the one I'm familiarized with.  \n",
    "There are various methods to make images have the same dimensions:\n",
    "   * resize to a fixed dimension\n",
    "   * add padding to some images and resize\n",
    "   * ...  \n",
    "\n",
    "In my other problem I have scanned pictures, so I trim the whitespace and resize afterwards. Being this a beginner-friendly dataset, all digits are the same size, binarized and well centered so no need to worry about resizing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac10231c13c9f52440bbb76ed6bc285c402d9b5e"
   },
   "source": [
    "## 2.10. One hot encoding of label\n",
    "\n",
    "At this point in the notebook the labels vary in the range [0-9] which is intuitive, but in order to define the type of loss for the NN later, which in this case is categorical_crossentropy (reason is explained in section 2), the targets should be in categorical format (=one hot-vectors): ex : 2 -> [0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "ytrain before  \n",
    "0    1  \n",
    "1    0  \n",
    "2    1  \n",
    "3    4  \n",
    "4    0  \n",
    "\n",
    "where the first column is the index,\n",
    "\n",
    "ytrain after  \n",
    "[[0. 1. 0. ... 0. 0. 0.]  \n",
    " [1. 0. 0. ... 0. 0. 0.]  \n",
    " [0. 1. 0. ... 0. 0. 0.]  \n",
    " ...  \n",
    " [0. 0. 0. ... 1. 0. 0.]  \n",
    " [0. 0. 0. ... 0. 0. 0.]  \n",
    " [0. 0. 0. ... 0. 0. 1.]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "508a23dfec7dde6ea68e2904277687283a956327"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Shape of ytrain before:  (42000,)\n",
      "Shape of ytrain after:  (42000, 10)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "print(type(ytrain))\n",
    "# number of classes, in this case 10\n",
    "nclasses = ytrain.max() - ytrain.min() + 1\n",
    "\n",
    "print(\"Shape of ytrain before: \", ytrain.shape) # (42000,)\n",
    "\n",
    "ytrain = to_categorical(ytrain, num_classes = nclasses)\n",
    "\n",
    "print(\"Shape of ytrain after: \", ytrain.shape) # (42000, 10), also numpy.ndarray type\n",
    "print(type(ytrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e078f5545f1387ae0b49f14fee7ac1d47e5bcae"
   },
   "source": [
    "## 2.11. Split training and validation sets\n",
    "\n",
    "The available data is 42k images. If the NN is trained with these 42k images, it might overfit and respond poorly to new data. Overfitting means that the NN doesn't generalize for the digits, it just learns the differences in those 42k images. When faced with new digits slightly different, the performance decreases considerably. This is not a good outcome, since the goal of the NN is to learn from the training set digits so that it does well on the **new digits**.\n",
    "\n",
    "In order to avoid submitting the predictions and risking a bad performance, and to determine whether the NN overfits, a small percentage of the train data is separated and named validation data. The ratio of the split can vary from 10% in small datasets to 1% in cases with 1M images.\n",
    "\n",
    "The NN is then trained with the remaining of the training data, and in each step/epoch, the NN is tested against the validation data and we can see its performance. That way we can watch how the loss and accuracy metrics vary during training, and in the end determine where there is overfitting and take action (more on this later). For example, the results I had after the 20th epoch with a certain CNN architecture which turned out to overfit:\n",
    "\n",
    "> loss: 0.0066 - acc: 0.9980 - val_loss: 0.0291 - val_acc: 0.9940\n",
    "\n",
    "In this example and without getting much into detail, the __training loss__ is very low while the __val_loss__ is 4 times higher, and the __training accuracy__ is a little higher than the __val_acc__. The accuracy difference is not that much, partly because we are talking about 0.998 vs 0.994, which is exceptionally high, but the difference in loss suggests an overfitting problem.\n",
    "\n",
    "Coming back to the general idea, the **val_acc** is the important metric. The NN might do very well with trained data but the goal is that the NN learns to generalize other than learning the training data \"by heart\". If the NN does well with val data, it's probable that it generalizes well to a certain extent and it will do well with the test data. (more on this in section 2 regarding CNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "631056d188bbeec02e02e7f564030b0f8d92178e"
   },
   "source": [
    "__random_state__ in train_test_split ensures that the data is pseudo-randomly divided.  \n",
    "If the images were ordered by class, activating this feature guarantees their pseudo-random split.  \n",
    "The seed means that every time this pseudo-randomization is applied, the distribution is the same.\n",
    "\n",
    "__stratify__ in train_test_split ensures that there is no overrepresentation of classes in the val set.  \n",
    "It is used to avoid some labels being overrepresented in the val set.   \n",
    "> Note: only works with sklearn version > 0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "117f9749a1511c0a238a0dece0f684bd27b41cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37800, 28, 28, 1) (37800, 10) (4200, 28, 28, 1) (4200, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "\n",
    "# percentage of xtrain which will be xval\n",
    "split_pct = 0.1\n",
    "\n",
    "# Split the train and the validation set\n",
    "xtrain, xval, ytrain, yval = train_test_split(xtrain,\n",
    "                                              ytrain, \n",
    "                                              test_size=split_pct,\n",
    "                                              random_state=seed,\n",
    "                                              shuffle=True,\n",
    "                                              stratify=ytrain\n",
    "                                             )\n",
    "\n",
    "print(xtrain.shape, ytrain.shape, xval.shape, yval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "237545703337e0e18dc1a7f40daa9edff5d9dec3"
   },
   "source": [
    "> Summary\n",
    "\n",
    "The available data is now divided as follows:\n",
    "* **Train data**: images (xtrain) and labels (ytrain), 90% of the available data\n",
    "* **Validation data**: images (xval) and labels (yval), 10% of the available data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6f751f76f859c7b51b846f81dc79baa7609e934"
   },
   "source": [
    "# 3. **CNN**\n",
    "\n",
    "In this section the CNN is defined, including architecture, optimizers, metrics, learning rate reductions, data augmentation... Then it is compiled and fit to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "162a9388b958cf25272870d290a616be47fd85ee"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# for the architecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Lambda, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPool2D, AvgPool2D\n",
    "\n",
    "# optimizer, data generator and learning rate reductor\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ee343a11f8881814ba57a48bdf1a69752c7cc34"
   },
   "source": [
    "## 3.1. Define model architecture\n",
    "\n",
    "Below is an example CNN architecture:\n",
    "\n",
    "![CNN architecture](https://cdn-images-1.medium.com/max/1000/1*xlOZHo8svfDWDyxFFnMurQ.png)\n",
    "\n",
    "My final CNN architechture is:\n",
    "> In &rarr; [ [Conv2D &rarr; relu]\\*2 &rarr; MaxPool2D &rarr; Dropout ]\\*2 &rarr; Flatten &rarr; Dense &rarr; Dropout &rarr; Out\n",
    "\n",
    "I'd like to encourage everyone who wants to learn about CNNs to begin with a simpler one, such as\n",
    "\n",
    "> In &rarr; [Conv2D &rarr; relu] &rarr; MaxPool2D &rarr; Flatten &rarr; Dense &rarr;  Out\n",
    "\n",
    ", check the performance and keep adding layers or tweaking the parameters until you reach an architecture (that may or may not be like mine) with a __val_acc__ of 0.996 more or less, trying to improve that takes much more time and it's really about the details, but of course feel free to try it out. I just encourage that you build your own model and do your own tests, instead of looking at an already well-performing model and using that.\n",
    "\n",
    "In my case I started with the simple architecture, kept a log where I wrote down the loss and accuracy results, changed one thing at a time, checked performance and how it changed regarding the previous version, wrote down the changes I had made and how the result changed, and made further changes based on that.\n",
    "\n",
    "More info on CNN architectures here: [How to choose CNN Architecture MNIST](https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f53f5e983c9e8769c12180ac9f1bc1c5a5d21fc3"
   },
   "source": [
    "### Architecture layers\n",
    "\n",
    "You can read about the theory of CNNs on the Internet from people more knowledgeable than me and who surely explain it much better. So I will skip the theory explanation for the Conv2D, MaxPool2D, Flatten and Dense layers and I will focus on smaller details.\n",
    "\n",
    "   * Conv2D\n",
    "   \n",
    "      * __filters__: usually on the first convolutional layers there are less filters, and more deeper down the CNN. Usually a power of 2 is set, and in this case 16 offered poorer performance and I didn't want to make a big CNN with 64 or 128 filters for digit classification.\n",
    "      \n",
    "      * __kernel_size__: this is the filter size, usually (3,3) or (5,5) is set. I advise setting one, building the architecture and changing it to see if it affects the performance though it usually doesn't.\n",
    "      \n",
    "      * __padding__: two options\n",
    "      \n",
    "         * valid padding: no padding, the image shrinks after convolution: n - f + 1\n",
    "         * same padding: padding of 2, the image doesn't shrink after convolution: p = (f-1)/2 &rarr; (n+2) - f(=3) + 1 = n\n",
    "         \n",
    "      * __activation__: ReLU is represented mathematically by max(0,X) and offers good performance in CNNs (source: the Internet)\n",
    "\n",
    "   * MaxPool2D: the goal is to reduce variance/overfitting and reduce computational complexity since it makes the image smaller. two pooling options\n",
    "   \n",
    "      * MaxPool2D: extracts the most important features like edges\n",
    "      * AvgPool2D: extracts smooth features\n",
    "      \n",
    "      My personal conclusion then is that for binarized images, with noticeable edge differences, MaxPool performs better.\n",
    "      \n",
    "      \n",
    "   * Dropout: you can read the theory on the Internet, it's a useful tool to reduce overfitting. The net becomes less sensitive to the specific weights of neurons and is more capable of better generalization and less likely to overfit to the train data. The optimal dropout value in Conv layers is 0.2, and if you want to implement it in the dense layers, its optimal value is 0.5: [Dropout in ML](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "bb1b773c6bf7baf33756dd265f3a2c1bc662b220"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0828 17:36:49.037243  6448 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0828 17:36:49.058158  6448 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0828 17:36:49.061149  6448 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0828 17:36:49.099049  6448 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0828 17:36:49.102042  6448 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0828 17:36:49.115005  6448 deprecation.py:506] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "dim = 28\n",
    "nclasses = 10\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(dim,dim,1)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu',))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(5,5), padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(5,5), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "model.add(Dense(nclasses, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "7983fec4500a27ff54adc40535405ba71e9fd80a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               376440    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 567,646\n",
      "Trainable params: 567,646\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "191698b8e851c0d7e5cde2eea6016fd865af3f55"
   },
   "source": [
    "This summary shows the summary of the model, displaying each layer with the shape of the output as well as the number of parameters it needs. The first dense layer is the one with the most parameters, since it maps the 3136 outputs of the Flatten layer to the 120 neurons of the Dense layer. Since the layer is a fully connected layer, the number of parameters is: 120 * 3136 + 120.\n",
    "\n",
    "The amount of trainable parameters is roughly half a million, which is not that much considering the architecture has medium size and the input dimensions (28,28,3) are small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5544fac92a5c0a32cf2431a69c092903a212f959"
   },
   "source": [
    "## 3.2. Compile the model\n",
    "\n",
    "   * **Optimizer**: it represents the gradient descent algorithm, whose goal is to minimize the cost function to approach the minimum point. **Adam** optimizer is one of the best-performing algorithms: [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8). The default learning rate for the Adam optimizer is 0.001. Another optimizer choice may be RMSprop or SGD.\n",
    "\n",
    "   * **Loss function**: It is a measure of the overall loss in the network after assigning values to the parameters during the forward phase so it indicates how well the parameters were chosen during the forward propagation phase. This loss function requires the labels to be encoded as one-hot vectors which is why this step was taken back in 1.8. \n",
    "\n",
    "   * **Metrics**: this refers to which metric the network should achieve, the most common one being 'accuracy' but there are other metrics to measure the performance other than accuracy, such as precision or recall or F1 score. The choice depends on the problem itself. Where high recall means low number of false negatives , High precision means low number of false positives and F1 score is a trade off between them: [Precision-Recall](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html). Depending on the problem, accuracy may not be the best metric. Suppose a binary classification problem where there are much more 0 values than 1, and therefore it's crucial that the predicted 1's are mostly correct. A network that just outputs 0 every time would get very high accuracy but the model still wouldn't perform well. Take the popular example:\n",
    "   \n",
    "   > A ML company has built a tool to identify terrorists among the population and they claim to have 99.99% accuracy. When inspecting their product, turns out they just output 0 in every case. Since there is only one terrorist for every 10000 people (this is made up, I actually have no idea what the probability is but all I know is that it's very low), the company has a very high precision, but there's no need of a ML tool for that. With the class imbalance being so high, accuracy is not a good metric anymore and other options should be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "5785f24f35271268d9616e45cdde37ddf4547921"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0828 17:36:51.881526  6448 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0828 17:36:51.912487  6448 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6bf54846016d6953c5eff1ea126b318a32206a51"
   },
   "source": [
    "## 3.3. Set other parameters \n",
    "\n",
    "### Learning rate annealer\n",
    "\n",
    "This is a useful tool which reduces the learning rate when there is a plateau on a certain value, which you can specify. In this case the monitoring value is __val_acc__. When there is no change in __val_acc__ in 3 epochs (patience), the learning rate is multiplied by 0.5 (factor). If the learning rate has the value of min_lr, it stops decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "e8a38a240404358164f63020aceb591c3907ffdc"
   },
   "outputs": [],
   "source": [
    "lr_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                 patience=3, \n",
    "                                 verbose=1, \n",
    "                                 factor=0.5, \n",
    "                                 min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ed29b562d85bb487fe56451436bfd65684bd6c6"
   },
   "source": [
    "### Data augmentation\n",
    "\n",
    "Data augmentation is a technique used to artificially make the training set bigger. There are a number of options for this, the most common ones include rotating images, zooming in a small range and shifting images horizontally and vertically. \n",
    "\n",
    "Beware that activating some features may be confusing for the network, imagine that when taking img1 and flipping it, it may be very similar to img2 which has a different label. With the digits 6 and 9 for example, if you take either and flip it vertically and horizontally, it becomes the other. So if you do that with the digit 9, flip it in both edges and tell the network that the digit is still a 9 when it actually is very similar to the images of the digit 6, the performance will drop considerably. So take into account the images and how activating the features may affect the labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "e1379cb5bd05278c3726b39df441a5eb1dcd9dc5"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "          featurewise_center=False,            # set input mean to 0 over the dataset\n",
    "          samplewise_center=False,             # set each sample mean to 0\n",
    "          featurewise_std_normalization=False, # divide inputs by std of the dataset\n",
    "          samplewise_std_normalization=False,  # divide each input by its std\n",
    "          zca_whitening=False,                 # apply ZCA whitening\n",
    "          rotation_range=30,                   # randomly rotate images in the range (degrees, 0 to 180)\n",
    "          zoom_range = 0.1,                    # Randomly zoom image \n",
    "          width_shift_range=0.1,               # randomly shift images horizontally (fraction of total width)\n",
    "          height_shift_range=0.1,              # randomly shift images vertically (fraction of total height)\n",
    "          horizontal_flip=False,               # randomly flip images\n",
    "          vertical_flip=False)                 # randomly flip images\n",
    "\n",
    "datagen.fit(xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ebea61c92df8ebe85c0fe42360775e1de002b76"
   },
   "source": [
    "### Epochs and batch_size\n",
    "\n",
    "   * **Epochs**: based on my experiments, the loss and accuracy get into a plateau at around the 10th epoch, so I usually set it to 15. \n",
    "   * **Batch_size**: I skip the theory which you can read it on the Internet. I recommend that you try changing it and seeing the change in the loss and accuracy, in my case a batch_size of 16 turned out to be disastrous and the best case occurred when I set it to 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "2fd7356e907ad7001a84890191271f59975ff49e"
   },
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "095c87f6e1063bbed5aa2ecfc51215768319491e"
   },
   "source": [
    "## 3.4  Fit the model\n",
    "\n",
    "Since there is data augmentation, the fitting function changes from fit (when there is no data augmentation) to fit_generator. The first input argument is slightly different. Otherwise you can specify the verbosity, number of epochs, validation data if any, any callbacks you want to include... This is one of the most time consuming cells in the notebook, and its running time depends on the number of epochs specified, number of trainable parameters in the network and input dimensions. Changing the batch_size also conrtibutes to changes in time, the bigger the batch_size, the faster the epoch.\n",
    "\n",
    "In a GPU such as the one Kaggle ofers, the training is done in 16s per epoch adding up to a total of 400s.\n",
    "\n",
    "> Note: remember to create and compile the model all over again whenever you change something, such as batch_size or epochs or anything related to the CNN. if you don't and just run the fit cell, it will continue training on the old network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "3a87fb83b3d6654ee0b2a54e65a8b9c00ec48cf4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0828 17:37:18.626373  6448 deprecation.py:323] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "590/590 [==============================] - 200s 339ms/step - loss: 0.3610 - acc: 0.8847 - val_loss: 0.0849 - val_acc: 0.9733\n",
      "Epoch 2/15\n",
      " 25/590 [>.............................] - ETA: 3:11 - loss: 0.1462 - acc: 0.9561"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0d7426323d88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                               callbacks=[lr_reduction])\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(xtrain,ytrain, batch_size=batch_size),\n",
    "                              epochs=epochs, \n",
    "                              validation_data=(xval,yval),\n",
    "                              verbose=1, \n",
    "                              steps_per_epoch=xtrain.shape[0] // batch_size, \n",
    "                              callbacks=[lr_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "556d85363f20ddeda657143c437327de757b8b23"
   },
   "source": [
    "## 3.5. Plot loss and accuracy\n",
    "\n",
    "After training the model, it's useful to plot the loss and accuracy in training and validation to see its progress and detect problems. In this particular case with this particular network, the training loss decreases, which means the network is learning, and there is no substantial difference between the training loss and validation loss wich indicates no overfitting. At this levels where the loss is so low and accuracy is so high there really is no bias or variance problem, but if you want to improve results further you could approach a bias problem, in other words, that the training loss is too high. To reduce this the recommended solutions are making a bigger network and training for a longer time. Feel free to tweak the network or epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5876a7e97bfb916020e9c04a6a6056bc96f98669",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"Validation loss\",axes =ax[0])\n",
    "ax[0].grid(color='black', linestyle='-', linewidth=0.25)\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n",
    "ax[1].grid(color='black', linestyle='-', linewidth=0.25)\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0577813580eb3ead164e8742e53768db2b176cbd"
   },
   "source": [
    "## 3.6. Plot confusion matrix\n",
    "\n",
    "I imported this from [yassineghouzam's kernel](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6). The confusion matrix is a nclasses x nclasses matrix (nclasses is the number of classes/labels in your classification problem).  The vertical axis shows the actual or true labels, while the horizontal axis shows the predicted labels, that is, the labels that the network has predicted.   \n",
    "\n",
    "In an ideal case, the matrix would be an identity matrix. All the points in the matrix would be 0 except the ones in the diagonal. This would happen if the network predicted the correct label every time and each label were predicted correctly every time, but it rarely happens. A more common situation is that there are many values in the diagonal, many occurrences of correctly labeled images, while there are some scattered wrong-labeled-images. \n",
    "\n",
    "In this case the wrong values seem to be randomly disrtibuted, which gives no information about how to proceed. If there were a bigger number of visible errors, such as the digit 4 being mistaken by the digit 9 several times, it would be intuitive to understand what is happening, since depending on how someone writes the digit 4, it might be very similar to 9.\n",
    "\n",
    "The case of the most popular digit, the digit 1,  is also noticeable (popular I mean there are 463 images corresponding to the digit 1, more than any other digit). In this case, the matrix shows that if the network predicts the digit 2, then it is always correct (the second column is all 0 except at digit 2), the network has perfect precision. However, the digit 2 is not always correctly labeled (second row), so the network doesn't have perfect recall. All in all, the more images available for a class, the less errors the network usually does. In an unrelated project I'm working with a small and imbalanced dataset and there's one class very under-represented and I'm having difficulties to predict it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63a0bda9010ac311a6a9c845343c64ebcba8ddc3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "ypred_onehot = model.predict(xval)\n",
    "# Convert predictions classes from one hot vectors to labels: [0 0 1 0 0 ...] --> 2\n",
    "ypred = np.argmax(ypred_onehot,axis=1)\n",
    "# Convert validation observations from one hot vectors to labels\n",
    "ytrue = np.argmax(yval,axis=1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(ytrue, ypred)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes=range(nclasses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bde5a08ce48c6d0602e8931a2d59dd76f99fd1d7"
   },
   "source": [
    "## 3.7. Plot errors\n",
    "\n",
    "Anoher useful approach is to plot the errors or wrongly labeled images, hoping it provides some intuition about what the network might be doing wrong. The cell outputs just 6 error images, you can change the range in the code to see other images. After running the cell, almost all images are very confusing and it would be difficult for a human to label them correctly. The third digit could be 1 or 7, the fourth a 0 or 6, the last could be either a 3 or a 8.. Even humans could make mistakes when labelling these images.\n",
    "\n",
    "Generally speaking, a network might have high bias (high training loss) and you could spend hours trying to decrease it, but the network may have reached the best human result. If the best digit-recognizer can only achieve 90% accuracy (assume that this is true for the sake of the example), the network won't be able to do much better. So when the network reaches this point, it's usually the limit of what it can do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f5f8df7abc5d6278caa4098f5d39ea7566a55ccb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors = (ypred - ytrue != 0) # array of bools with true when there is an error or false when the image is cor\n",
    "\n",
    "ypred_er = ypred_onehot[errors]\n",
    "ypred_classes_er = ypred[errors]\n",
    "ytrue_er = ytrue[errors]\n",
    "xval_er = xval[errors]\n",
    "\n",
    "def display_errors(errors_index, img_errors, pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "            \n",
    "# Probabilities of the wrong predicted numbers\n",
    "ypred_er_prob = np.max(ypred_er,axis=1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_er = np.diagonal(np.take(ypred_er, ytrue_er, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_er = ypred_er_prob - true_prob_er\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_delta_er = np.argsort(delta_pred_true_er)\n",
    "\n",
    "# Top 6 errors. You can change the range to see other images\n",
    "most_important_er = sorted_delta_er[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_er, xval_er, ypred_classes_er, ytrue_er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04ece1be5e18b0bbda311c49d6e3566d49460452"
   },
   "source": [
    "# 4. **Predict and save to csv**\n",
    "\n",
    "Once you are happy with your network, use the test data to create the prediction, this cell creates the csv format necessary to submit to the [Digit Recognizer](https://www.kaggle.com/c/digit-recognizer). \n",
    "\n",
    "> Remember that the csv is only created when you Commit the kernel, not when you run the cell in the editor. Make sure you commit the kernel, that it is successfull, and then in the kernel page (clicking the left arrows on the top left), there will be a tab called __Output__ where you can find your csv and submit it directly to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f85a38f96f2fb9c26119f6a2a32ce84ee0751c91",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test, verbose=1)\n",
    "\n",
    "submissions = pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
    "                            \"Label\": predictions})\n",
    "\n",
    "submissions.to_csv(\"mnist2908.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "580778ade72465b34fd1665ca0871326fe71b855"
   },
   "source": [
    "And that's it! I hope the explanations were informative and the code was clean and well-formatted. If there are any comments or suggestions whatsoever, if something is not clear or if you have any (positive) criticism towards the kernel, don't hesitate to tell me! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
